{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "180 Degrees Consulting Bristol            |  Binder\n",
    ":-------------------------:|:-------------------------:\n",
    "![](https://180dc.org/wp-content/uploads/2017/06/180-logo1.jpg)  |  ![](https://mybinder.org/static/images/logo_social.png)\n",
    "\n",
    "# Donation Analysis\n",
    "\n",
    "Welcome to this Jupyter Notebook. Below you will see code that will perform some\n",
    "\n",
    "- Exploratory Analysis\n",
    "- Data Visualisation\n",
    "- Machine Learning\n",
    "\n",
    "on some (fake) donation data. Important - as data is made up it is quite weird.\n",
    "\n",
    "You can run each cell by selecting it and clicking run in the toolbar above. Cells sometimes need to be run in order.\n",
    "\n",
    "Below is a cell where the data is imported and then manipulated into a Pandas Dataframe from which we can do some analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "donor_data = pd.read_csv(\"donor_data.csv\")\n",
    "\n",
    "ids = []\n",
    "dates = []\n",
    "types = []\n",
    "events = []\n",
    "amounts = []\n",
    "quarters = [] # new feature created for association analysis\n",
    "\n",
    "def which_quarter(x):\n",
    "    if x <= 91:\n",
    "        return \"first\"\n",
    "    elif 91 < x and x <= 182:\n",
    "        return \"second\"\n",
    "    elif 182 < x and x <= 273:\n",
    "        return \"third\"\n",
    "    else:\n",
    "        return \"fourth\"\n",
    "\n",
    "for key, value in donor_data.iterrows():\n",
    "    donation = value[0]\n",
    "    \n",
    "    donor_id = donation.split(\"[\")[0].replace(\" \", \"\")\n",
    "    date = donation.split(\"[\")[1].split(\",\")[0].replace(\" \", \"\")\n",
    "    donation_type = donation.split(\"[\")[1].split(\",\")[1].replace(\" \", \"\")\n",
    "    event = donation.split(\"[\")[1].split(\",\")[2][1:].replace(\" \", \"\")\n",
    "    amount = donation.split(\"[\")[1].split(\",\")[3][:-1].replace(\" \", \"\")\n",
    "    quarter = which_quarter(int(date))\n",
    "    \n",
    "    ids.append(donor_id)\n",
    "    dates.append(date)\n",
    "    types.append(donation_type)\n",
    "    events.append(event)\n",
    "    amounts.append(amount)\n",
    "    quarters.append(quarter)\n",
    "    \n",
    "donor_data = pd.DataFrame({\"id\" : ids, \"date\" : dates, \"type\" : types,\n",
    "                           \"event\" : events, \"amount\" : amounts,\n",
    "                           \"quarter\" : quarters})\n",
    "\n",
    "donor_data[[\"id\", \"date\", \"amount\"]] = donor_data[[\"id\", \"date\", \"amount\"]].apply(pd.to_numeric)\n",
    "donor_data.set_index(\"id\", inplace=True)\n",
    "\n",
    "donor_data.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis\n",
    "\n",
    "We'll take each variable and see what we can find.\n",
    "\n",
    "### Date\n",
    "To simplify things the dates run from 1 to 365 representing the days in a normal calendar year. Let's have a look at donation frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# change figure size\n",
    "sns.set(rc={'figure.figsize':(11, 4)})\n",
    "\n",
    "ax = donor_data[\"date\"].hist(bins=20)\n",
    "ax.set(title = \"Donation Frequency Histogram\", \n",
    "       xlabel = \"Day\", ylabel=\"Number of Donations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to note:\n",
    "\n",
    "- slow start to the year\n",
    "- peak number of donations around 140 days, 200 days, 300 days\n",
    "\n",
    "Later we'll look at events to see if that can explain the patterns we see here.\n",
    "\n",
    "Let's have a look at the donations not linked to any event (also not legacy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Donation Frequency Histogram (No Event, Legacy Removed)\"\n",
    "\n",
    "ax = donor_data[(donor_data[\"event\"] == \"N\") & \n",
    "                (donor_data[\"type\"] != \"Legacy\")][\"date\"].hist(bins=20)\n",
    "\n",
    "ax.set(title = title, xlabel = \"Day\", ylabel = \"Number of Donations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much more uniform however still peaks - suggests people may make donations due to attending events but not always specify the event when making the donation.\n",
    "\n",
    "Lets look at the amounts data.\n",
    "\n",
    "### Amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(3, figsize=(11, 10))\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "donor_data[\"amount\"].hist(ax=axs[0])\n",
    "\n",
    "donor_data[donor_data[\"type\"] != \"Legacy\"][\"amount\"].hist(ax=axs[1])\n",
    "\n",
    "donor_data[(donor_data[\"type\"] != \"Legacy\") &\n",
    "           (donor_data[\"amount\"] < 50)][\"amount\"].hist(ax=axs[2])\n",
    "\n",
    "axs[0].set(title = \"Amounts Histogram\", \n",
    "           xlabel = \"Amount (£)\", ylabel = \"Frequency\")\n",
    "axs[1].set(title = \"Amounts Histogram (Legacy Donations Removed)\", \n",
    "           xlabel = \"Amount (£)\", ylabel = \"Frequency\")\n",
    "axs[2].set(title = \"Amounts Histogram (Legacy Donations and >= 50 Removed)\", \n",
    "           xlabel = \"Amount (£)\", ylabel = \"Frequency\")\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donation amounts data can be tricky. It's massively skewed towards smaller donations. Additionally legacy donations can screw with results as they are so large. It's common to remove them when conducting further analysis as they work in very different ways to other donations.\n",
    "\n",
    "Let's check for an exponential distribution then create sensible buckets in case it's necessary for further analysis.\n",
    "\n",
    "We can check for an exponential distribution by calculating the ML estimate of the exponential distribution then comparing quantiles of our data vs this distribution and seeing if they match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "amounts_non_legacy = donor_data[donor_data[\"type\"] != \"Legacy\"][\"amount\"]\n",
    "\n",
    "lambda_hat = 1/amounts_non_legacy.mean()\n",
    "\n",
    "plot = stats.probplot(amounts_non_legacy, dist=stats.expon(scale=lambda_hat), plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":( well exponential doesn't look like a good fit - you'll have to take my word that it normally is.\n",
    "\n",
    "Anyway for the buckets lets do something like this:\n",
    "\n",
    "0-10\n",
    "10-100\n",
    "100-1000\n",
    "1000-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = []\n",
    "\n",
    "for amount in donor_data[\"amount\"]:\n",
    "    if amount < 10:\n",
    "        sizes.append(\"small\")\n",
    "    elif (amount >= 10) & (amount < 100):\n",
    "        sizes.append(\"medium\")\n",
    "    elif (amount >= 100) & (amount < 1000):\n",
    "        sizes.append(\"large\")\n",
    "    else:\n",
    "        sizes.append(\"huge\")\n",
    "        \n",
    "donor_data[\"size\"] = sizes\n",
    "\n",
    "donor_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Events\n",
    "\n",
    "First let's find out what events there are and when they occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donor_data[\"event\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These correspond respectively to Bristol 10k, No event, Christmas Meal and Summer Fete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donor_data[\"event\"].value_counts().plot(kind='bar').set(title=\"Events\", ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Donation Frequency Histogram with Events overlayed\"\n",
    "\n",
    "sns.histplot(donor_data, x=\"date\", hue=\"event\", multiple=\"stack\").set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graphs we see B10K generates the most donations and seems to occur around day 150. We can also infer that the summer fete occurs around day 200 and the Christmas meal around day 300.\n",
    "\n",
    "### Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donor_data[\"type\"].value_counts().plot(kind='bar').set(title=\"Donation Type\", ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeated donations are good. We want to find associations between features and repeated donations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "explanatory_features = [\"quarter\", \"event\", \"size\"]\n",
    "\n",
    "for feature in explanatory_features:\n",
    "    obs = pd.crosstab(donor_data[\"type\"], donor_data[feature])\n",
    "    chi2 = chi2_contingency(obs)\n",
    "    \n",
    "    print(feature + \" vs type:    test-statistic = {0:.2f}\".format(chi2[0]) + \n",
    "          \"    p-value = {0:.2f}\".format(chi2[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there isn't significant association between quarter and type but there is between event and type. Let's take a closer look at the contingency table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(donor_data[\"type\"], donor_data[\"event\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the contingency table we see most repeated donations come from no donations and the summer fete is the best generator of repeated donations.\n",
    "\n",
    "Let's move on to something more complicated - machine learning.\n",
    "\n",
    "## Machine Learning (Classifcation with Logistic Regression)\n",
    "\n",
    "We're gonna look at predicting whether a donation is repeated or not, then analyse the coefficients to see if we can discover what influences this.\n",
    "\n",
    "Here's the process:\n",
    "\n",
    "- Remove legacy donations\n",
    "- Convert categorical explanatory variables into dummy variables\n",
    "- Create response variable (1 for repeated, 0 for single)\n",
    "- Split into train and test data\n",
    "- Check model performance\n",
    "- Analyse Coefficients\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = donor_data[donor_data[\"type\"] != \"Legacy\"]\n",
    "\n",
    "X = pd.get_dummies(X[['event','quarter','size']])\n",
    "\n",
    "y = []\n",
    "\n",
    "for i in donor_data[\"type\"]:\n",
    "    if i == \"Single\":\n",
    "        y.append(0)\n",
    "    elif i == \"Repeated\":\n",
    "        y.append(1)\n",
    "\n",
    "print(X.head())\n",
    "print(y[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_percent = 0.8\n",
    "n_rows = round(len(X)*t_percent)\n",
    "\n",
    "Xtrain = X[:n_rows]\n",
    "Xtest = X[n_rows:]\n",
    "\n",
    "ytrain = y[:n_rows]\n",
    "ytest = y[n_rows:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(Xtrain, ytrain)\n",
    "\n",
    "ytest_hat = logreg.predict(Xtest)\n",
    "\n",
    "print(classification_report(ytest, ytest_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most influential in favor of repeated: 3rd variable and 10th variable\n",
    "Most influential against repeated: first variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bristol 10k not good at generating repeated donations.\n",
    "- Most repeated donations not linked to an event.\n",
    "- Most repeated donations were of a medium size.\n",
    "- Quarter not particuarly impactful\n",
    "\n",
    "## Summary\n",
    "\n",
    "During this interactive session we looked at\n",
    "\n",
    "- Pandas Dataframes\n",
    "- Histograms and bar charts with Seaborn\n",
    "- Donation amount distribution and the impact of legacy donations\n",
    "- ML estimators and QQ plots to check quality of fit\n",
    "- Complex graph showing impact of events on donation frequency across the year\n",
    "- Association analysis and hypothesis testing with scipy\n",
    "- Machine learning process - specifically logistic regression with scikit-learn\n",
    "\n",
    "How much better is this than Excel? :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
